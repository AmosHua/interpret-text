{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of SST-2 Sentences using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`**. This will run the notebook on a small subset of the data and a use a smaller number of epochs. \n",
    "\n",
    "If you run into CUDA out-of-memory error or the jupyter kernel dies constantly, try reducing the `BATCH_SIZE` and `MAX_LEN`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-yohwa\\AppData\\Local\\Continuum\\anaconda3\\envs\\interpret_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\v-yohwa\\AppData\\Local\\Continuum\\anaconda3\\envs\\interpret_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\v-yohwa\\AppData\\Local\\Continuum\\anaconda3\\envs\\interpret_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\v-yohwa\\AppData\\Local\\Continuum\\anaconda3\\envs\\interpret_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\v-yohwa\\AppData\\Local\\Continuum\\anaconda3\\envs\\interpret_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\v-yohwa\\AppData\\Local\\Continuum\\anaconda3\\envs\\interpret_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scrapbook as sb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils_nlp.dataset.multinli import load_pandas_df \n",
    "from utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.common.timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_text.msra.MSRAExplainer import MSRAExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a pretrained [BERT](https://arxiv.org/abs/1810.04805) model on a subset of the [SST-2](https://nlp.stanford.edu/sentiment/index.html/) dataset.\n",
    "\n",
    "We use a [sequence classifier](https://github.com/microsoft/nlp/blob/master/utils_nlp/models/bert/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-pretrained-BERT) of Google's [BERT](https://github.com/google-research/bert)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters\n",
    "Here we set some parameters that we use for our modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FRACTION = 1\n",
    "TEST_DATA_FRACTION = 1\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_FRACTION = 0.01\n",
    "    TEST_DATA_FRACTION = 0.01\n",
    "    NUM_EPOCHS = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 32\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "DATA_FOLDER = \"./temp/sst2\"\n",
    "BERT_CACHE_DIR = \"./temp/sst2\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE_PRED = 512\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"labels\" \n",
    "TEXT_COL = \"sentences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder.\n",
    "\n",
    "The SST-2 dataset is dataset of Rotten Tomatoes movie reviews mainly used for natural language inference (NLI) tasks, where the inputs are sentences and the labels are binary (positive or negative) sentiment indicators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath):\n",
    "    df_dict = {LABEL_COL: [], TEXT_COL: []}\n",
    "    with open(fpath, 'r') as f:\n",
    "        label_start = 0\n",
    "        sentence_start = 2\n",
    "        for line in f:\n",
    "            label = int(line[label_start])\n",
    "            sentence = line[sentence_start:]\n",
    "            df_dict['labels'].append(label)\n",
    "            df_dict['sentences'].append(sentence)\n",
    "    return pd.DataFrame.from_dict(df_dict)\n",
    "\n",
    "df_train = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.train'))\n",
    "df_test = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "\n",
    "\n",
    "if QUICK_RUN:\n",
    "    df_train = df_train.sample(frac=TRAIN_DATA_FRACTION).reset_index(drop=True)\n",
    "    df_test = df_test.sample(frac=TEST_DATA_FRACTION).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the two classes in the dataset, where \"1\" corresponds to a positive review and \"0\" corresponds to a negative review. We don't need to encode the labels as they are already integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels                                          sentences\n",
      "0       0  i have not been this disappointed by a movie i...\n",
      "1       1  the way the roundelay of partners functions , ...\n",
      "2       0  by no means a slam-dunk and sure to ultimately...\n",
      "3       0  the movie 's biggest shocks come from seeing f...\n",
      "4       1     cool gadgets and creatures keep this fresh .\\n\n",
      "0    41\n",
      "1    28\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# display stats and examples for label types\n",
    "print(df_train[[LABEL_COL, TEXT_COL]].head())\n",
    "print(df_train[LABEL_COL].value_counts())\n",
    "\n",
    "# create training and testing labels\n",
    "labels_train = df_train[LABEL_COL]\n",
    "labels_test = df_test[LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n",
      "Number of training examples: 69\n",
      "Number of testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we tokenize the text documents and convert them to lists of tokens. The following steps instantiate a `BERT tokenizer` given the language, and tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                       | 0/69 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69/69 [00:00<00:00, 1816.37it/s]\n",
      "  0%|                                                                                                                                       | 0/18 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 1500.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'have', 'not', 'been', 'this', 'disappointed', 'by', 'a', 'movie', 'in', 'a', 'long', 'time', '.'], ['the', 'way', 'the', 'round', '##ela', '##y', 'of', 'partners', 'functions', ',', 'and', 'the', 'inter', '##play', 'within', 'partnerships', 'and', 'among', 'partnerships', 'and', 'the', 'general', 'air', 'of', 'ga', '##tor', '-', 'bash', '##ing', 'are', 'consistently', 'delightful', '.'], ['by', 'no', 'means', 'a', 'slam', '-', 'dun', '##k', 'and', 'sure', 'to', 'ultimately', 'di', '##sa', '##pp', '##oint', 'the', 'action', 'fans', 'who', 'will', 'be', 'moved', 'to', 'the', 'edge', 'of', 'their', 'seats', 'by', 'the', 'dynamic', 'first', 'act', ',', 'it', 'still', 'comes', 'off', 'as', 'a', 'touching', ',', 'trans', '##cend', '##ent', 'love', 'story', '.'], ['the', 'movie', \"'\", 's', 'biggest', 'shocks', 'come', 'from', 'seeing', 'former', 'ny', '##mp', '##hett', '##e', 'juliette', 'lewis', 'playing', 'a', 'salt', '-', 'of', '-', 'the', '-', 'earth', 'mommy', 'named', 'minnie', 'and', 'watching', 'slim', 'travel', 'inc', '##og', '##ni', '##to', 'in', 'a', 'ridiculous', 'wig', 'no', 'respectable', 'halloween', 'costume', 'shop', 'would', 'ever', 'try', 'to', 'sell', '.'], ['cool', 'ga', '##dgets', 'and', 'creatures', 'keep', 'this', 'fresh', '.'], ['a', 'movie', 'that', 'ca', 'n', \"'\", 't', 'get', 'sufficient', 'distance', 'from', 'leroy', \"'\", 's', 'del', '##usions', 'to', 'escape', 'their', 'maud', '##lin', 'influence', '.'], ['i', 'ca', 'n', \"'\", 't', 'say', 'this', 'enough', ':', 'this', 'movie', 'is', 'about', 'an', 'adult', 'male', 'dressed', 'in', 'pink', 'jam', '##mies', '.'], ['argent', '##o', ',', 'at', 'only', '26', ',', 'brings', 'a', 'youthful', ',', 'out', '-', 'to', '-', 'change', '-', 'the', '-', 'world', 'aggressive', '##ness', 'to', 'the', 'project', ',', 'as', 'if', 'she', \"'\", 's', 'cut', 'open', 'a', 'vein', 'and', 'bled', 'the', 'raw', 'film', 'stock', '.'], ['this', 'thing', 'works', 'on', 'no', 'level', 'whatsoever', 'for', 'me', '.'], ['the', 'cast', '.', '.', '.', 'keeps', 'this', 'pretty', 'watch', '##able', ',', 'and', 'casting', 'mick', 'jagger', 'as', 'director', 'of', 'the', 'escort', 'service', 'was', 'inspired', '.'], ['little', 'action', ',', 'almost', 'no', 'suspense', 'or', 'bel', '##ie', '##vable', 'tension', ',', 'one', '-', 'dimensional', 'characters', 'up', 'the', 'wa', '##zoo', 'and', 'sets', 'that', 'can', 'only', 'be', 'described', 'as', 'sci', '-', 'fi', 'generic', '.'], ['say', '##les', 'has', 'an', 'eye', 'for', 'the', 'ways', 'people', 'of', 'different', 'ethnic', '##ities', 'talk', 'to', 'and', 'about', 'others', 'outside', 'the', 'group', '.'], ['a', 'bizarre', 'piece', 'of', 'work', ',', 'with', 'premise', 'and', 'dialogue', 'at', 'the', 'level', 'of', 'kids', \"'\", 'television', 'and', 'plot', 'threads', 'as', 'mor', '##ose', 'as', 'teen', 'pregnancy', ',', 'rape', 'and', 'suspected', 'murder'], ['the', 'film', 'falls', 'short', 'on', 'tension', ',', 'el', '##o', '##que', '##nce', ',', 'spiritual', 'challenge', '-', '-', 'things', 'that', 'have', 'made', 'the', 'original', 'new', 'testament', 'stories', 'so', 'compelling', 'for', '20', 'centuries', '.'], ['like', 'the', 'rug', '##rat', '##s', 'movies', ',', 'the', 'wild', 'thorn', '##berry', '##s', 'movie', 'does', 'n', \"'\", 't', 'offer', 'much', 'more', 'than', 'the', 'series', ',', 'but', 'its', 'emphasis', 'on', 'caring', 'for', 'animals', 'and', 'respecting', 'other', 'cultures', 'is', 'particularly', 'welcome', '.'], ['the', 'otherwise', 'good', '-', 'nature', '##d', '##ness', 'of', 'mr', '.', 'deeds', ',', 'with', 'its', 'embrace', 'of', 'sheer', 'goo', '##fine', '##ss', 'and', 'cameo', '##s', 'of', 'less', '-', 'than', '-', 'likely', 'new', 'york', 'celebrities', '.', '.', '.', 'certainly', 'raises', 'the', 'film', 'above', 'anything', 'sand', '##ler', \"'\", 's', 'been', 'attached', 'to', 'before', '.'], ['a', 'gr', '##ating', '##ly', 'un', '##fu', '##nn', '##y', 'groan', '##er', 'littered', 'with', 'zero', '-', 'dimensional', ',', 'un', '##lika', '##ble', 'characters', 'and', 'hackney', '##ed', ',', 'thread', '##bar', '##e', 'comic', 'setup', '##s', '.'], ['the', 'soul', '-', 'searching', 'deliberate', '##ness', 'of', 'the', 'film', ',', 'although', 'leave', '##ned', 'nicely', 'with', 'dry', 'absurd', '##ist', 'wit', ',', 'eventually', 'becomes', 'too', 'heavy', 'for', 'the', 'plot', '.'], ['absurd', '##ities', 'and', 'cl', '##ich', '##a', '##©', '##s', 'accumulate', 'like', 'lin', '##t', 'in', 'a', 'fat', 'man', \"'\", 's', 'nave', '##l', '.'], ['but', 'the', 'nerve', '-', 'raked', 'acting', ',', 'the', 'crack', '##le', 'of', 'lines', ',', 'the', 'impressive', 'staging', '##s', 'of', 'hardware', ',', 'make', 'for', 'some', 'robust', 'and', 'scary', 'entertainment', '.'], ['good', 'actors', 'have', 'a', 'radar', 'for', 'juicy', 'roles', '-', '-', 'there', \"'\", 's', 'a', 'pl', '##eth', '##ora', 'of', 'characters', 'in', 'this', 'picture', ',', 'and', 'not', 'one', 'of', 'them', 'is', 'flat', '.'], ['the', 'film', 'is', 'hampered', 'by', 'its', 'predictable', 'plot', 'and', 'paper', '-', 'thin', 'supporting', 'characters', '.'], ['.', '.', '.', 'an', 'interesting', 'slice', 'of', 'history', '.'], ['enough', 'may', 'pan', '##der', 'to', 'our', 'bases', '##t', 'desires', 'for', 'pay', '##back', ',', 'but', 'unlike', 'many', 'revenge', 'fantasies', ',', 'it', 'ultimately', 'delivers', '.'], ['ni', '##jin', '##sky', 'says', ',', \"'\", 'i', 'know', 'how', 'to', 'suffer', \"'\", 'and', 'if', 'you', 'see', 'this', 'film', 'you', \"'\", 'll', 'know', 'too', '.'], ['it', \"'\", 's', 'pretty', 'linear', 'and', 'only', 'makeup', '-', 'deep', ',', 'but', 'bog', '##dan', '##ovich', 'ties', 'it', 'together', 'with', 'efficiency', 'and', 'an', 'affection', 'for', 'the', 'period', '.'], ['as', 'a', 'story', 'of', 'dramatic', 'enlightenment', ',', 'the', 'screenplay', 'by', 'billy', 'ray', 'and', 'terry', 'george', 'leaves', 'something', 'to', 'be', 'desired', '.'], ['nair', 'just', 'does', 'n', \"'\", 't', 'have', 'the', 'necessary', 'self', '-', 'control', 'to', 'guide', 'a', 'loose', ',', 'poorly', 'structured', 'film', 'through', 'the', 'pit', '##falls', 'of', 'inc', '##oh', '##erence', 'and', 'red', '##unda', '##ncy', '.'], ['special', 'p', '.', 'o', '.', 'v', '.', 'camera', 'mounts', 'on', 'bikes', ',', 'skate', '##boards', ',', 'and', 'motorcycles', 'provide', 'an', 'intense', 'experience', 'when', 'splashed', 'across', 'the', 'immense', 'im', '##ax', 'screen', '.'], ['all', 'right', ',', 'so', 'it', \"'\", 's', 'not', 'a', 'brilliant', 'piece', 'of', 'filmmaking', ',', 'but', 'it', 'is', 'a', 'funny', '-', 'l', '##rb', '-', 'sometimes', 'hilarious', '-', 'rr', '##b', '-', 'comedy', 'with', 'a', 'def', '##t', 'sense', 'of', 'humor', 'about', 'itself', ',', 'a', 'playful', 'spirit', 'and', 'a', 'game', 'cast', '.'], ['too', 'much', 'of', 'nemesis', 'has', 'a', 'tired', ',', 'talk', '##y', 'feel', '.'], ['what', 'sour', '##ed', 'me', 'on', 'the', 'santa', 'clause', '2', 'was', 'that', 'santa', 'bumps', 'up', 'against', '21st', 'century', 'reality', 'so', 'hard', ',', 'it', \"'\", 's', 'ic', '##ky', '.'], ['the', 'skills', 'of', 'a', 'calculus', 'major', 'at', 'm', '.', 'i', '.', 't', '.', 'are', 'required', 'to', 'balance', 'all', 'the', 'formula', '##ic', 'equations', 'in', 'the', 'long', '-', 'wind', '##ed', 'he', '##ist', 'comedy', 'who', 'is', 'cl', '##eti', '##s', 'to', '##ut', '?'], ['for', 'its', '100', 'minutes', 'running', 'time', ',', 'you', \"'\", 'll', 'wait', 'in', 'vain', 'for', 'a', 'movie', 'to', 'happen', '.'], ['hard', 'as', 'this', 'may', 'be', 'to', 'believe', ',', 'here', 'on', 'earth', ',', 'a', 'surprisingly', 'similar', 'teen', 'drama', ',', 'was', 'a', 'better', 'film', '.'], ['instead', 'of', 'using', 'george', 'and', 'lucy', \"'\", 's', 'most', 'obvious', 'differences', 'to', 'ign', '##ite', 'sparks', ',', 'lawrence', 'desperately', 'looks', 'elsewhere', ',', 'seizing', 'on', 'george', \"'\", 's', 'ha', '##ples', '##s', '##ness', 'and', 'lucy', \"'\", 's', 'personality', 'ti', '##cs', '.'], ['were', 'dylan', 'thomas', 'alive', 'to', 'witness', 'first', '-', 'time', 'director', 'ethan', 'hawke', \"'\", 's', 'strained', 'chelsea', 'walls', ',', 'he', 'might', 'have', 'been', 'tempted', 'to', 'change', 'his', 'landmark', 'poem', 'to', ',', '`', 'do', 'not', 'go', 'gentle', 'into', 'that', 'good', 'theatre', '.', \"'\"], ['un', '##sp', '##eak', '##able', ',', 'of', 'course', ',', 'barely', 'begins', 'to', 'describe', 'the', 'plot', 'and', 'its', 'complications', '.'], ['an', 'ex', '##cr', '##uc', '##iating', 'demonstration', 'of', 'the', 'un', '##sal', '##va', '##ge', '##ability', 'of', 'a', 'movie', 'saddle', '##d', 'with', 'an', 'amateur', '##ish', 'screenplay', '.'], ['a', 'conventional', ',', 'but', 'well', '-', 'crafted', 'film', 'about', 'a', 'historic', 'legal', 'battle', 'in', 'ireland', 'over', 'a', 'man', \"'\", 's', 'right', 'to', 'raise', 'his', 'own', 'children', '.'], ['wander', '##s', 'all', 'over', 'the', 'map', 'thematic', '##ally', 'and', 'stylistic', '##ally', ',', 'and', 'borrow', '##s', 'heavily', 'from', 'lynch', ',', 'je', '##une', '##t', ',', 'and', 'von', 'trier', 'while', 'failing', 'to', 'find', 'a', 'spark', 'of', 'its', 'own', '.'], ['o', '##ede', '##ker', '##k', 'mug', '##s', 'mer', '##ci', '##lessly', ',', 'and', 'the', 'genuinely', 'funny', 'jokes', 'are', 'few', 'and', 'far', 'between', '.'], ['it', \"'\", 's', 'very', 'bea', '##vis', 'and', 'butt', '##head', ',', 'yet', 'always', 'seems', 'to', 'eli', '##cit', 'a', 'chuckle', '.'], ['de', '##uce', '##s', 'wild', 'tread', '##s', 'heavily', 'into', 'romeo', 'and', 'juliet', '\\\\', '/', 'west', 'side', 'story', 'territory', ',', 'where', 'it', 'plainly', 'has', 'no', 'business', 'going', '.'], ['devoid', 'of', 'any', 'of', 'the', 'qualities', 'that', 'made', 'the', 'first', 'film', 'so', 'special', '.'], ['.', '.', '.', 'the', 'story', 'is', 'far', '-', 'flung', ',', 'ill', '##ogical', ',', 'and', 'plain', 'stupid', '.'], ['this', 'is', 'pure', ',', 'exciting', 'movie', '##making', '.'], ['-', 'l', '##rb', '-', 'jane', '##y', '-', 'rr', '##b', '-', 'forget', '##s', 'about', 'her', 'other', 'obligations', ',', 'leading', 'to', 'a', 'tragedy', 'which', 'is', 'somehow', 'guess', '##able', 'from', 'the', 'first', 'few', 'minutes', ',', 'maybe', 'because', 'it', 'echoes', 'the', 'by', 'now', 'into', '##ler', '##able', 'mor', '##bid', '##ity', 'of', 'so', 'many', 'recent', 'movies', '.'], ['it', \"'\", 's', 'as', 'raw', 'and', 'action', '-', 'packed', 'an', 'experience', 'as', 'a', 'rings', '##ide', 'seat', 'at', 'a', 'tough', '-', 'man', 'contest', '.'], ['-', 'l', '##rb', '-', 'denis', \"'\", '-', 'rr', '##b', '-', 'bare', '-', 'bones', 'narrative', 'more', 'closely', 'resembles', 'an', 'outline', 'for', 'a', \"'\", '70s', 'exploitation', 'picture', 'than', 'the', 'finished', 'product', '.'], ['it', \"'\", 's', 'a', 'mystery', 'how', 'the', 'movie', 'could', 'be', 'released', 'in', 'this', 'condition', '.'], ['while', 'you', 'have', 'to', 'admit', 'it', \"'\", 's', 'semi', '-', 'amusing', 'to', 'watch', 'robert', 'den', '##iro', 'belt', 'out', '`', '`', 'when', 'you', \"'\", 're', 'a', 'jet', ',', 'you', \"'\", 're', 'a', 'jet', 'all', 'the', 'way', ',', \"'\", \"'\", 'it', \"'\", 's', 'equally', 'di', '##sta', '##ste', '##ful', 'to', 'watch', 'him', 'sing', 'the', 'lyrics', 'to', '`', '`', 'tonight', '.', \"'\", \"'\"], ['this', '10th', 'film', 'in', 'the', 'series', 'looks', 'and', 'feels', 'tired', '.'], ['what', 'saves', 'this', 'deeply', 'affecting', 'film', 'from', 'being', 'merely', 'a', 'collection', 'of', 'wren', '##ching', 'cases', 'is', 'co', '##rc', '##uer', '##a', \"'\", 's', 'attention', 'to', 'detail', '.'], ['oh', ',', 'look', 'at', 'that', 'clever', 'angle', '!'], ['feels', 'aim', '##less', 'for', 'much', 'of', 'its', 'running', 'time', ',', 'until', 'late', 'in', 'the', 'film', 'when', 'a', 'tidal', 'wave', 'of', 'plot', 'arrives', ',', 'leaving', 'questions', 'in', 'its', 'wake', '.'], ['if', 'we', 'sometimes', 'need', 'comforting', 'fantasies', 'about', 'mental', 'illness', ',', 'we', 'also', 'need', 'movies', 'like', 'tim', 'mccann', \"'\", 's', 'revolution', 'no', '.', '9', '.'], ['a', 'small', 'independent', 'film', 'suffering', 'from', 'a', 'severe', 'case', 'of', 'hollywood', '-', 'it', '##is', '.'], ['of', 'all', 'the', 'halloween', \"'\", 's', ',', 'this', 'is', 'the', 'most', 'visually', 'una', '##ppe', '##aling', '.'], ['for', 'decades', 'we', \"'\", 've', 'marvel', '##ed', 'at', 'disney', \"'\", 's', 'rendering', 'of', 'water', ',', 'snow', ',', 'flames', 'and', 'shadows', 'in', 'a', 'hand', '-', 'drawn', 'animated', 'world', '.'], ['gay', 'or', 'straight', ',', 'kissing', 'jessica', 'stein', 'is', 'one', 'of', 'the', 'greatest', 'date', 'movies', 'in', 'years', '.'], ['a', 'sleep', '-', 'inducing', '##ly', 'slow', '-', 'paced', 'crime', 'drama', 'with', 'clumsy', 'dialogue', ',', 'heavy', '-', 'handed', 'phone', '##y', '-', 'feeling', 'sentiment', ',', 'and', 'an', 'overly', '-', 'familiar', 'set', 'of', 'plot', 'devices', '.'], ['nick', '##s', 'refuses', 'to', 'let', 'slack', '##ers', 'be', 'seen', 'as', 'just', 'another', 'teen', 'movie', ',', 'which', 'means', 'he', 'can', 'be', 'forgiven', 'for', 'frequently', 'pan', '##der', '##ing', 'to', 'fans', 'of', 'the', 'gross', '-', 'out', 'comedy', '.'], ['if', 'it', \"'\", 's', 'un', '##ner', '##ving', 'suspense', 'you', \"'\", 're', 'after', '-', '-', 'you', \"'\", 'll', 'find', 'it', 'with', 'ring', ',', 'an', 'ind', '##is', '##put', '##ably', 'sp', '##ook', '##y', 'film', ';', 'with', 'a', 'screenplay', 'to', 'die', 'for', '.'], ['sh', '##rew', '##d', 'but', 'pointless', '.'], ['both', 'garcia', 'and', 'jagger', 'turn', 'in', 'perfectly', 'executed', 'and', 'wonderful', '##ly', 'sympathetic', 'characters', ',', 'who', 'are', 'alternately', 'touching', 'and', 'funny', '.'], ['com', '##pu', '##ls', '##ively', 'watch', '##able', ',', 'no', 'matter', 'how', 'degraded', 'things', 'get', '.'], ['the', 'sight', 'of', 'the', 'spaceship', 'on', 'the', 'launching', 'pad', 'is', 'duly', 'impressive', 'in', 'im', '##ax', 'dimensions', ',', 'as', 'are', 'shots', 'of', 'the', 'astronauts', 'floating', 'in', 'their', 'cabins', '.'], ['the', 'new', 'film', 'of', 'anton', 'che', '##khov', \"'\", 's', 'the', 'cherry', 'orchard', 'puts', 'the', '`', 'ic', '##k', \"'\", 'in', '`', 'classic', '.', \"'\"]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)\n",
    "\n",
    "tokens_train = tokenizer.tokenize(list(df_train[TEXT_COL]))\n",
    "tokens_test = tokenizer.tokenize(list(df_test[TEXT_COL]))\n",
    "\n",
    "print(tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we perform the following preprocessing steps in the cell below:\n",
    "- Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "- Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence, respectively\n",
    "- Pad or truncate the token lists to the specified max length. In this case, `MAX_LEN = 150`\n",
    "- Return mask lists that indicate the paddings' positions\n",
    "- Return token type id lists that indicate which sentence the tokens belong to (not needed for one-sequence classification)\n",
    "\n",
    "*See the original [implementation](https://github.com/google-research/bert/blob/master/run_classifier.py) for more information on BERT's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, mask_train, _ = tokenizer.preprocess_classification_tokens(tokens_train, MAX_LEN)\n",
    "tokens_test, mask_test, _ = tokenizer.preprocess_classification_tokens(tokens_test, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classifier Model\n",
    "Next, we use a sequence classifier that loads a pre-trained BERT model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTSequenceClassifier(language=LANGUAGE, num_labels=num_labels, cache_dir=BERT_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We train the classifier using the training set. This involves fine-tuning the BERT Transformer and learning a linear classification layer on top of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - t_total value of -1 results in schedule not being applied\n",
      "\n",
      "\n",
      "Iteration:   0%|                                                                                                                             | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:1->1/9; average training loss:0.691921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  11%|████████████▉                                                                                                       | 1/9 [04:29<35:56, 269.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:2->2/9; average training loss:0.696667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  22%|█████████████████████████▊                                                                                          | 2/9 [08:59<31:27, 269.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:3->3/9; average training loss:0.645818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  33%|██████████████████████████████████████▋                                                                             | 3/9 [13:24<26:49, 268.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:4->4/9; average training loss:0.680287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  44%|███████████████████████████████████████████████████▌                                                                | 4/9 [17:30<21:48, 261.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:5->5/9; average training loss:0.709238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  56%|████████████████████████████████████████████████████████████████▍                                                   | 5/9 [21:25<16:54, 253.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:6->6/9; average training loss:0.721394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  67%|█████████████████████████████████████████████████████████████████████████████▎                                      | 6/9 [25:21<12:24, 248.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:7->7/9; average training loss:0.709737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  78%|██████████████████████████████████████████████████████████████████████████████████████████▏                         | 7/9 [29:16<08:08, 244.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:8->8/9; average training loss:0.700559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  89%|████████████████████████████████████████████████████████████████████████████████████████████████▉            | 8/9 [16:35:07<4:52:36, 17556.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/1; batch:9->9/9; average training loss:0.701680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [16:39:00<00:00, 12359.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training time: 16.656 hrs]\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(token_ids=tokens_train,\n",
    "                    input_mask=mask_train,\n",
    "                    labels=labels_train,    \n",
    "                    num_epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,    \n",
    "                    verbose=True)    \n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Model\n",
    "We score the test set using the trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:   0%|                                                                                                                             | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [06:22<00:00, 382.93s/it]"
     ]
    }
   ],
   "source": [
    "preds = classifier.predict(token_ids=tokens_test, \n",
    "                           input_mask=mask_test, \n",
    "                           batch_size=BATCH_SIZE_PRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Finally, we compute the overall accuracy, precision, recall, and F1 metrics on the test set. We also look at the metrics for eact of the genres in the the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7222222222222222\n",
      "{\n",
      "    \"0\": {\n",
      "        \"f1-score\": 0.8387096774193548,\n",
      "        \"precision\": 0.7222222222222222,\n",
      "        \"recall\": 1.0,\n",
      "        \"support\": 13\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"f1-score\": 0.0,\n",
      "        \"precision\": 0.0,\n",
      "        \"recall\": 0.0,\n",
      "        \"support\": 5\n",
      "    },\n",
      "    \"macro avg\": {\n",
      "        \"f1-score\": 0.4193548387096774,\n",
      "        \"precision\": 0.3611111111111111,\n",
      "        \"recall\": 0.5,\n",
      "        \"support\": 18\n",
      "    },\n",
      "    \"micro avg\": {\n",
      "        \"f1-score\": 0.7222222222222222,\n",
      "        \"precision\": 0.7222222222222222,\n",
      "        \"recall\": 0.7222222222222222,\n",
      "        \"support\": 18\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"f1-score\": 0.6057347670250895,\n",
      "        \"precision\": 0.5216049382716049,\n",
      "        \"recall\": 0.7222222222222222,\n",
      "        \"support\": 18\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(labels_test, preds, target_names=label_encoder.classes_, output_dict=True) \n",
    "accuracy = accuracy_score(labels_test, preds)\n",
    "# change labels in report to strings for ease of display\n",
    "report[\"0\"] = report.pop(0)\n",
    "report[\"1\"] = report.pop(1)\n",
    "\n",
    "print(\"accuracy: {}\".format(accuracy))\n",
    "print(json.dumps(report, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.7222222222222222,
       "encoder": "json",
       "name": "accuracy",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "accuracy"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.3611111111111111,
       "encoder": "json",
       "name": "precision",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "precision"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.5,
       "encoder": "json",
       "name": "recall",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "recall"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.4193548387096774,
       "encoder": "json",
       "name": "f1",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "f1"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for testing\n",
    "sb.glue(\"accuracy\", accuracy)\n",
    "sb.glue(\"precision\", report[\"macro avg\"][\"precision\"])\n",
    "sb.glue(\"recall\", report[\"macro avg\"][\"recall\"])\n",
    "sb.glue(\"f1\", report[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "\n",
    "classifier.model.to(device)\n",
    "for param in classifier.model.parameters():\n",
    "    param.requires_grad = False\n",
    "classifier.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_msra = MSRAExplainer(model=classifier.model, \n",
    "                                 train_dataset=list(df_train[TEXT_COL]), \n",
    "                                 device=device, \n",
    "                                 target_layer=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. wedge and mr. saldanha handle the mix of verbal jokes and slapstick well .\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "text = df_test[TEXT_COL][1]\n",
    "label = df_test[LABEL_COL][1]\n",
    "print(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                                       | 0/69 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69/69 [00:00<00:00, 683.15it/s]"
     ]
    }
   ],
   "source": [
    "explanation_msra = interpreter_msra.explain_local(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_msra.visualize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (interpret_cpu)",
   "language": "python",
   "name": "interpret_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
